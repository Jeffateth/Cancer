{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "← Reusing cached SpatialData.\n",
      "← Reusing cached H&E image and pixel sizes.\n",
      "← Using provided SpatialData and H&E image, skipping load_data.\n",
      "→ Filtering cells based on total counts using MAD …\n",
      "→ Applying MAD-based filtering on total_counts\n",
      "✔ 147707 cells kept (≥ 67.0 total_counts)\n",
      "✔ 147707 cells kept (≥ 67.0 transcripts)\n",
      "→ Extracting centroids from SpatialData geometry …\n",
      "[DEBUG] Entering extract_xenium_nucleus_centroids_from_spatialdata\n",
      "[DEBUG] Available shape keys: ['cell_boundaries', 'cell_circles', 'nucleus_boundaries']\n",
      "[DEBUG] nucleus_shapes contains 190965 geometries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c0/44p3b4xx4v956042lj_vp7z80000gn/T/ipykernel_69764/4117260420.py:184: DeprecationWarning: Table accessor will be deprecated with SpatialData version 0.1, use sdata.tables instead.\n",
      "  cell_table = sdata.table if hasattr(sdata, 'table') else sdata['table']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Extracted 190965 nucleus centroids\n",
      "[DEBUG] Exiting extract_xenium_nucleus_centroids_from_spatialdata\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'centroids_um' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 590\u001b[0m\n\u001b[1;32m    588\u001b[0m sdata_cache, he_image_cache, psx_cache, psy_cache \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Then reuse caches on subsequent calls\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmain_patch_extraction_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43msdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msdata_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhe_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhe_image_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpsx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpsx_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpsy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpsy_cache\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 490\u001b[0m, in \u001b[0;36mmain_patch_extraction_pipeline\u001b[0;34m(sdata, he_image, psx, psy)\u001b[0m\n\u001b[1;32m    456\u001b[0m all_centroids_um, all_cell_ids \u001b[38;5;241m=\u001b[39m extract_xenium_nucleus_centroids_from_spatialdata(sdata)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m# Print debug information about cell ID formats \u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03mprint(f\"→ Debug: First few valid_cell_ids: {valid_cell_ids[:3]}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    raise ValueError(\"No cells matched between filtered AnnData and nucleus boundaries. Check cell ID formats.\")\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✔ Extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(centroids_um)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m valid centroids after MAD filtering\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Step 3: Align Xenium coordinates to H&E image\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Use the updated alignment function that checks for existing transformation\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m→ Starting alignment process…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'centroids_um' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "COMPREHENSIVE ANNOTATION:\n",
    "Cell Patch Extraction Pipeline for Xenium Spatial Transcriptomics Data\n",
    "\n",
    "This code performs extraction and quality assessment of cell patches from H&E (Hematoxylin and Eosin) \n",
    "stained histology images, aligned with spatial transcriptomics data from the Xenium platform. \n",
    "The goal is to extract image patches centered around cell nuclei detected in Xenium data.\n",
    "\n",
    "Key functionalities:\n",
    "1. Loading spatial transcriptomics data and corresponding H&E images\n",
    "2. Converting nucleus coordinates from microns to pixels\n",
    "3. Sampling cells in a spatially distributed manner\n",
    "4. Extracting image patches at multiple zoom levels (kappa values)\n",
    "5. Evaluating the quality of extracted patches\n",
    "6. Selecting the best patches based on quality metrics\n",
    "\n",
    "The pipeline serves as a preprocessing step for downstream analysis that combines \n",
    "histological features with gene expression data at single-cell resolution.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import xml.etree.ElementTree as ET  # For parsing XML metadata in OME-TIFF files\n",
    "\n",
    "# Numerical and data processing libraries\n",
    "import numpy as np  # For efficient numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "\n",
    "# Image processing libraries\n",
    "from scipy import ndimage  # For image transformations like distance transforms\n",
    "from skimage import exposure, filters, measure, color, transform  # For advanced image processing\n",
    "from tifffile import TiffFile  # For reading TIFF files with metadata\n",
    "import spatialdata as sd  # Library for spatial transcriptomics data\n",
    "\n",
    "from tqdm.notebook import tqdm  # Progress bar for Jupyter notebooks\n",
    "\n",
    "# =============================================================================\n",
    "# Module-level cache for heavy data\n",
    "# =============================================================================\n",
    "# Why use module-level caching? \n",
    "# Answer: Loading large spatial datasets and high-resolution images is computationally expensive.\n",
    "# Caching these objects prevents redundant loading operations, significantly improving performance\n",
    "# when functions are called multiple times.\n",
    "# _cached_sdata = None  # Cache for SpatialData object\n",
    "# _cached_he_image = None  # Cache for H&E image\n",
    "# _cached_pixel_size = None  # Cache for pixel size (psx, psy) in microns\n",
    "\n",
    "# Paths to data files\n",
    "# Why hardcode paths?\n",
    "# Answer: While not ideal for production, hardcoding simplifies development and testing.\n",
    "# In a production environment, these would be parameterized or loaded from configuration.\n",
    "base_dir      = \"/Users/jianzhouyao/Cancer\"\n",
    "xenium_dir    = os.path.join(base_dir, \"Pancreatic Cancer with Xenium Human Multi-Tissue and Cancer Panel\")\n",
    "zarr_path     = os.path.join(xenium_dir, \"Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zarr\")\n",
    "he_image_path = os.path.join(xenium_dir, \"Xenium_V1_hPancreas_Cancer_Add_on_FFPE_he_image.ome.tif\")\n",
    "\n",
    "# =============================================================================\n",
    "# Data loading with caching\n",
    "# =============================================================================\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load SpatialData and H&E image (with OME metadata) once and cache globally.\n",
    "    \n",
    "    Why use this function pattern?\n",
    "    Answer: It implements a lazy loading pattern with caching. Data is loaded only when needed\n",
    "    and stored for future use, preventing redundant loading operations.\n",
    "    \n",
    "    Returns:\n",
    "        sdata: SpatialData object containing spatial transcriptomics data\n",
    "        he_image: H&E image as ndarray (Height, Width, Channels)\n",
    "        psx, psy: pixel size in microns (physical dimensions of each pixel)\n",
    "    \"\"\"\n",
    "    global _cached_sdata, _cached_he_image, _cached_pixel_size\n",
    "    _cached_sdata = globals().get(\"_cached_sdata\", None)\n",
    "    _cached_he_image = globals().get(\"_cached_he_image\", None)\n",
    "    _cached_pixel_size = globals().get(\"_cached_pixel_size\", None)\n",
    "    \n",
    "    # Load SpatialData if not already cached\n",
    "    if _cached_sdata is None:\n",
    "        print(\"→ Loading SpatialData Zarr …\")\n",
    "        _cached_sdata = sd.read_zarr(zarr_path)  # Load data from Zarr storage format\n",
    "        print(\"✔ SpatialData loaded.\")\n",
    "    else:\n",
    "        print(\"← Reusing cached SpatialData.\")\n",
    "\n",
    "    # Load H&E image and metadata if not already cached\n",
    "    if _cached_he_image is None or _cached_pixel_size is None:\n",
    "        print(\"→ Loading H&E TIFF (memmap) & reading OME metadata …\")\n",
    "        \n",
    "        # Why use memmap?\n",
    "        # Answer: Memory mapping allows files to be accessed without loading the entire file into\n",
    "        # memory, which is crucial for large image files that might exceed available RAM.\n",
    "        with TiffFile(he_image_path) as tif:\n",
    "            img = tif.asarray(out='memmap')  # Load image as memory-mapped array\n",
    "            ome_xml = tif.ome_metadata  # Extract OME metadata\n",
    "        \n",
    "        # Why check and potentially transpose the image?\n",
    "        # Answer: Standardizes the image dimensions to have channels as the last dimension (H,W,C),\n",
    "        # regardless of how they were stored in the TIFF file (which might be C,H,W).\n",
    "        if img.ndim == 3 and img.shape[0] in (1, 3):\n",
    "            img = np.moveaxis(img, 0, -1)  # Convert from (C,H,W) to (H,W,C) format\n",
    "        \n",
    "        # Parse OME-XML metadata to extract pixel size information\n",
    "        # Why use ElementTree and namespaces?\n",
    "        # Answer: OME-TIFF follows a standardized XML schema with specific namespaces.\n",
    "        # Using ElementTree with appropriate namespaces ensures correct parsing.\n",
    "        root = ET.fromstring(ome_xml)\n",
    "        ns = {'ns': 'http://www.openmicroscopy.org/Schemas/OME/2016-06'}\n",
    "        pixels = root.find('.//ns:Pixels', ns)\n",
    "        \n",
    "        # Extract physical pixel size in microns\n",
    "        # Why default to 1.0?\n",
    "        # Answer: Ensures the code doesn't fail if the metadata is missing.\n",
    "        # A value of 1.0 implies 1 pixel = 1 micron as a fallback.\n",
    "        psx = float(pixels.attrib.get('PhysicalSizeX', 1.0))\n",
    "        psy = float(pixels.attrib.get('PhysicalSizeY', 1.0))\n",
    "        \n",
    "        # Store loaded data in cache\n",
    "        _cached_he_image = img\n",
    "        _cached_pixel_size = (psx, psy)\n",
    "        \n",
    "        print(f\"✔ H&E loaded (shape={img.shape}), pixel sizes: X={psx} µm, Y={psy} µm\")\n",
    "    else:\n",
    "        print(\"← Reusing cached H&E image and pixel sizes.\")\n",
    "        img = _cached_he_image\n",
    "        psx, psy = _cached_pixel_size\n",
    "\n",
    "    return _cached_sdata, img, psx, psy\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# QC & preprocessing functions\n",
    "# -----------------------------------------------------------------------------\n",
    "def paper_specific_qc(patches, qr):\n",
    "    \"\"\"\n",
    "    Apply additional quality control specific to paper requirements.\n",
    "    \n",
    "    Why add paper-specific QC?\n",
    "    Answer: This addresses issues specific to the analysis pipeline or publication\n",
    "    requirements that weren't covered by the general QC. In this case, it removes\n",
    "    completely black patches at the largest zoom level.\n",
    "    \n",
    "    Parameters:\n",
    "        patches: Dictionary of patches per cell\n",
    "        qr: Quality results\n",
    "        \n",
    "    Returns:\n",
    "        Updated patches and quality results\n",
    "    \"\"\"\n",
    "    # Identify completely black patches at kappa=2.0\n",
    "    to_rm = []\n",
    "    for cid in patches:\n",
    "        for i,q in enumerate(qr[cid]):\n",
    "            if q['kappa']==2.0 and patches[cid][i].mean()<5:\n",
    "                to_rm.append((cid,i))\n",
    "    \n",
    "    print(f\"Removing {len(to_rm)} completely black κ=2.0 patches\")\n",
    "    \n",
    "    # Mark identified patches as invalid\n",
    "    for cid,i in to_rm:\n",
    "        qr[cid][i]['is_valid']=False\n",
    "        qr[cid][i]['reason']=\"Completely black patch\"\n",
    "    \n",
    "    return patches, qr\n",
    "\n",
    "def mad_filter_cells(sdata, column=\"total_counts\", verbose=True):\n",
    "    \"\"\"\n",
    "    Filter cells based on MAD (Median Absolute Deviation) of a given column.\n",
    "    \n",
    "    Parameters:\n",
    "        sdata: SpatialData object\n",
    "        column: Column name in sdata.table.obs to apply MAD filtering on\n",
    "        verbose: Whether to print status messages\n",
    "    \n",
    "    Returns:\n",
    "        valid_cell_ids: Numpy array of filtered cell IDs\n",
    "        threshold: Threshold value used for filtering\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"→ Applying MAD-based filtering on\", column)\n",
    "\n",
    "    # Extract AnnData table\n",
    "    cell_table = sdata.table if hasattr(sdata, 'table') else sdata['table']\n",
    "\n",
    "    if column not in cell_table.obs.columns:\n",
    "        raise ValueError(f\"Missing '{column}' column in SpatialData table.\")\n",
    "\n",
    "    counts = cell_table.obs[column].values\n",
    "    median_C = np.median(counts)\n",
    "    mad_C = np.median(np.abs(counts - median_C))\n",
    "    threshold = median_C - mad_C\n",
    "    keep_mask = counts >= threshold\n",
    "\n",
    "    valid_cell_table = cell_table[keep_mask]\n",
    "    valid_cell_ids = valid_cell_table.obs_names.to_numpy()\n",
    "\n",
    "    n_valid = keep_mask.sum()\n",
    "    if n_valid == 0:\n",
    "        raise ValueError(\"No cells passed MAD filtering. Consider adjusting the threshold.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"✔ {n_valid} cells kept (≥ {threshold:.1f} {column})\")\n",
    "    print(f\"✔ {n_valid} cells kept (≥ {threshold:.1f} transcripts)\")\n",
    "    return valid_cell_ids, threshold\n",
    "\n",
    "def extract_xenium_nucleus_centroids_from_spatialdata(sdata):\n",
    "    \"\"\"\n",
    "    Extract nucleus centroids from a SpatialData object containing Xenium data.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering extract_xenium_nucleus_centroids_from_spatialdata\")\n",
    "    print(f\"[DEBUG] Available shape keys: {list(sdata.shapes.keys())}\")\n",
    "\n",
    "    # Try to get nucleus boundaries or cell circles to extract centroids\n",
    "    if 'nucleus_boundaries' in sdata.shapes:\n",
    "        nucleus_shapes = sdata.shapes['nucleus_boundaries']\n",
    "        print(f\"[DEBUG] nucleus_shapes contains {len(nucleus_shapes)} geometries\")\n",
    "        nucleus_centroids = np.array([\n",
    "            (geom.centroid.x, geom.centroid.y) \n",
    "            for geom in nucleus_shapes.geometry\n",
    "        ])\n",
    "        cell_ids = nucleus_shapes.index.values\n",
    "        print(f\"[DEBUG] Extracted {nucleus_centroids.shape[0]} nucleus centroids\")\n",
    "\n",
    "    elif 'cell_circles' in sdata.shapes:\n",
    "        print(\"[DEBUG] Using 'cell_circles' from sdata.shapes\")\n",
    "        cell_circles = sdata.shapes['cell_circles']\n",
    "        print(f\"[DEBUG] cell_circles contains {len(cell_circles)} geometries\")\n",
    "        nucleus_centroids = np.array([\n",
    "            (geom.centroid.x, geom.centroid.y) \n",
    "            for geom in cell_circles.geometry\n",
    "        ])\n",
    "        cell_ids = cell_circles.index.values\n",
    "        print(f\"[DEBUG] Extracted {nucleus_centroids.shape[0]} cell circle centroids as nucleus centroids\")\n",
    "\n",
    "    else:\n",
    "        print(\"[DEBUG] Neither 'nucleus_boundaries' nor 'cell_circles' found. Trying 'spatial' in obsm...\")\n",
    "        if 'spatial' in sdata.tables['table'].obsm:\n",
    "            nucleus_centroids = sdata.tables['table'].obsm['spatial']\n",
    "            cell_ids = sdata.tables['table'].obs_names.values\n",
    "            print(f\"[DEBUG] Extracted {nucleus_centroids.shape[0]} centroids from obsm['spatial']\")\n",
    "        else:\n",
    "            raise ValueError(\"Could not find nucleus centroids in the SpatialData object\")\n",
    "\n",
    "    print(\"[DEBUG] Exiting extract_xenium_nucleus_centroids_from_spatialdata\")\n",
    "    return nucleus_centroids, cell_ids\n",
    "\n",
    "def sample_grid_cells(centroids, grid_size=8, cells_per_grid=5):\n",
    "    \"\"\"\n",
    "    Sample cells from a spatial grid for balanced representation.\n",
    "    \n",
    "    Why use grid sampling?\n",
    "    Answer: Random sampling might oversample dense regions and undersample sparse ones. -> some regions may have many detected cells (high density), while others are sparse\n",
    "    If you randomly sample cells, you'll likely pick many from dense regions and miss spatially interesting areas.\n",
    "    Grid sampling samples cells evenly across space using a grid.\n",
    "    \n",
    "    Parameters:\n",
    "        centroids: Array of (x,y) coordinates\n",
    "        grid_size: Number of grid divisions in each dimension -> Default = 8 → 8×8 = 64 grid cells.\n",
    "        cells_per_grid: Maximum cells to sample from each grid cell -> Default = 5\n",
    "        \n",
    "    Returns:\n",
    "        Array of indices of sampled cells\n",
    "    \"\"\"\n",
    "    xs, ys = centroids[:,0], centroids[:,1]\n",
    "    xmin,xmax, ymin,ymax = xs.min(), xs.max(), ys.min(), ys.max()\n",
    "    \n",
    "    # Calculate grid cell sizes\n",
    "    dx, dy = (xmax-xmin)/grid_size, (ymax-ymin)/grid_size\n",
    "    \n",
    "    out = []\n",
    "    # Iterate through each grid cell\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            # Find cells within current grid cell\n",
    "            m = ((xs>=xmin+i*dx)&(xs<xmin+(i+1)*dx)&\n",
    "                 (ys>=ymin+j*dy)&(ys<ymin+(j+1)*dy))\n",
    "            idx = np.where(m)[0]\n",
    "            \n",
    "            # Sample cells from current grid cell\n",
    "            if idx.size:\n",
    "                take = min(cells_per_grid, idx.size)\n",
    "                out.extend(np.random.choice(idx, take, replace=False))\n",
    "    \n",
    "    return np.array(out)\n",
    "\n",
    "# =============================================================================\n",
    "# Fast downsample preview via strided slicing\n",
    "# =============================================================================\n",
    "def create_downsampled_preview(he_image, coords_px, sample_indices, factor=10):\n",
    "    \"\"\"\n",
    "    Create a downsampled preview of the H&E image with sampled cell locations.\n",
    "    \n",
    "    Why create a downsampled preview?\n",
    "    Answer: Full-resolution H&E images can be extremely large (gigapixels). A downsampled\n",
    "    preview allows quick visual verification of coordinate mapping and sampling without\n",
    "    requiring excessive memory or computation.\n",
    "    \n",
    "    Parameters:\n",
    "        he_image: Full H&E image\n",
    "        coords_px: Pixel coordinates of all cells\n",
    "        sample_indices: Indices of sampled cells\n",
    "        factor: Downsampling factor\n",
    "    \"\"\"\n",
    "    h, w = he_image.shape[:2]\n",
    "    small_h, small_w = h // factor, w // factor\n",
    "    print(f\"→ Fast downsample to ({small_h}, {small_w}) via slicing …\")\n",
    "    \n",
    "    # Downsample image via strided slicing\n",
    "    # Why use slicing instead of interpolation?\n",
    "    # Answer: Much faster for large images, and sufficient for preview purposes\n",
    "    small_image = he_image[::factor, ::factor, :].astype(np.uint8)\n",
    "    \n",
    "    # Scale down coordinates to match downsampled image\n",
    "    pts = coords_px[sample_indices] // factor\n",
    "    x_pts, y_pts = pts[:, 0], pts[:, 1]\n",
    "    \n",
    "    # Visualize sampled points on downsampled image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(small_image)\n",
    "    plt.scatter(x_pts, y_pts, c='red', s=10, alpha=0.7)\n",
    "    \n",
    "    # Label first 20 points for reference\n",
    "    for i, idx in enumerate(sample_indices[:20]):\n",
    "        x, y = coords_px[idx] // factor\n",
    "        plt.text(x + 5, y + 5, str(i), color='white', fontsize=9,\n",
    "                 bbox=dict(facecolor='black', alpha=0.7))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import estimate_transform, AffineTransform\n",
    "import napari\n",
    "\n",
    "def align_xenium_to_he(he_image, xenium_coords):\n",
    "    \"\"\"\n",
    "    Manually pick landmarks using Napari to align Xenium coordinates to the H&E image.\n",
    "\n",
    "    Parameters:\n",
    "        he_image (ndarray): H&E image (H, W) or (H, W, C)\n",
    "        xenium_coords (ndarray): Xenium centroid coordinates (N, 2)\n",
    "\n",
    "    Returns:\n",
    "        aligned_coords (ndarray): Transformed Xenium coordinates aligned to H&E space (N, 2)\n",
    "        transform_matrix (ndarray): 3x3 affine transformation matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Open napari viewer\n",
    "    viewer = napari.Viewer()\n",
    "    viewer.add_image(he_image, name=\"H&E Image\")\n",
    "    viewer.add_points(xenium_coords, name=\"Xenium Coordinates\", size=10, face_color=\"red\")\n",
    "\n",
    "    # Create empty layers for user to pick landmarks\n",
    "    he_points_layer = viewer.add_points(name=\"H&E Landmarks\", size=10, face_color=\"blue\")\n",
    "    xenium_points_layer = viewer.add_points(name=\"Xenium Landmarks\", size=10, face_color=\"green\")\n",
    "\n",
    "    print(\"\\n💡 INSTRUCTIONS:\")\n",
    "    print(\"- Use the 'H&E Landmarks' layer to click landmarks on the H&E image.\")\n",
    "    print(\"- Use the 'Xenium Landmarks' layer to click corresponding points in Xenium space.\")\n",
    "    print(\"- Pick the same number of points in the same order.\")\n",
    "    print(\"- Close the napari window to continue.\\n\")\n",
    "\n",
    "    napari.run()\n",
    "\n",
    "    # Retrieve landmarks\n",
    "    he_landmarks = he_points_layer.data\n",
    "    xenium_landmarks = xenium_points_layer.data\n",
    "\n",
    "    if len(he_landmarks) != len(xenium_landmarks):\n",
    "        raise ValueError(f\"❌ Mismatched landmarks: {len(he_landmarks)} (H&E) vs {len(xenium_landmarks)} (Xenium)\")\n",
    "\n",
    "    # Estimate affine transformation\n",
    "    transform = estimate_transform('affine', src=xenium_landmarks, dst=he_landmarks)\n",
    "\n",
    "    # Apply to all Xenium coordinates\n",
    "    aligned_coords = transform(xenium_coords)\n",
    "\n",
    "    print(\"✅ Transformation complete. Xenium coordinates aligned to H&E image.\")\n",
    "\n",
    "    return aligned_coords, transform.params\n",
    "\n",
    "def apply_saved_transform(transform_path, new_coords_path, output_path):\n",
    "    \"\"\"\n",
    "    Apply a saved affine transformation to new Xenium coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        transform_path (str): Path to the saved transformation matrix (CSV).\n",
    "        new_coords_path (str): Path to the new Xenium coordinates (CSV).\n",
    "        output_path (str): Path to save the aligned coordinates (CSV).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load saved transform matrix\n",
    "    loaded_matrix = np.loadtxt(transform_path, delimiter=\",\")\n",
    "    transform = AffineTransform(matrix=loaded_matrix)\n",
    "\n",
    "    # Load new Xenium coordinates\n",
    "    new_coords = np.loadtxt(new_coords_path, delimiter=\",\")\n",
    "\n",
    "    # Apply the transformation\n",
    "    aligned_new_coords = transform(new_coords)\n",
    "\n",
    "    # Save the aligned coordinates\n",
    "    np.savetxt(output_path, aligned_new_coords, delimiter=\",\")\n",
    "    print(f\"Aligned coordinates saved to {output_path}\")\n",
    "    \n",
    "# =============================================================================\n",
    "# Main pipeline with optional preloading\n",
    "# =============================================================================\n",
    "# Import the alignment functions\n",
    "from xenium_he_aligner import align_xenium_to_he, apply_transform\n",
    "\n",
    "def main_patch_extraction_pipeline(sdata=None, he_image=None, psx=None, psy=None):\n",
    "    \"\"\"\n",
    "    Main function that orchestrates the entire patch extraction pipeline.\n",
    "    \n",
    "    Why have a comprehensive pipeline function?\n",
    "    Answer: Organizes the entire workflow in a logical sequence, making it easier to:\n",
    "    1. Understand the complete process\n",
    "    2. Identify bottlenecks\n",
    "    3. Reuse the pipeline with different datasets\n",
    "    4. Optionally skip steps by providing preloaded data\n",
    "    \n",
    "    Parameters:\n",
    "        sdata: Optional preloaded SpatialData object\n",
    "        he_image: Optional preloaded H&E image\n",
    "        psx, psy: Optional preloaded pixel sizes\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing all extracted data and results\n",
    "    \"\"\"\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Load data if not provided\n",
    "    if sdata is None or he_image is None or psx is None or psy is None:\n",
    "        sdata, he_image, psx, psy = load_data()\n",
    "    else:\n",
    "        print(\"← Using provided SpatialData and H&E image, skipping load_data.\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Step 1: Filter cells based on total counts using MAD\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"→ Filtering cells based on total counts using MAD …\")\n",
    "    valid_cell_ids, threshold = mad_filter_cells(sdata)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Step 2: Extract centroids of valid cells from SpatialData geometry\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"→ Extracting centroids from SpatialData geometry …\")\n",
    "\n",
    "    # Use helper function to get all centroids\n",
    "    all_centroids_um, all_cell_ids = extract_xenium_nucleus_centroids_from_spatialdata(sdata)\n",
    "    \"\"\"\n",
    "    # Print debug information about cell ID formats \n",
    "    print(f\"→ Debug: First few valid_cell_ids: {valid_cell_ids[:3]}\")\n",
    "    print(f\"→ Debug: First few all_cell_ids: {all_cell_ids[:3]}\")\n",
    "    print(f\"→ Debug: Types - valid_cell_ids: {type(valid_cell_ids[0])}, all_cell_ids: {type(all_cell_ids[0])}\")\n",
    "\n",
    "    # Debug: Print lengths of valid and all cell IDs\n",
    "    print(f\"→ Debug: Number of valid_cell_ids: {len(valid_cell_ids)}\")\n",
    "    print(f\"→ Debug: Number of all_cell_ids: {len(all_cell_ids)}\")\n",
    "\n",
    "    # Ensure consistent ID format (convert both to lowercase strings and strip whitespace)\n",
    "    valid_cell_ids_set = set(str(cid).strip().lower() for cid in valid_cell_ids)\n",
    "    all_cell_ids_str = np.array([str(cid).strip().lower() for cid in all_cell_ids])\n",
    "\n",
    "    # Debug: Print a few IDs after formatting\n",
    "    print(f\"→ Debug: First few formatted valid_cell_ids: {list(valid_cell_ids_set)[:3]}\")\n",
    "    print(f\"→ Debug: First few formatted all_cell_ids: {all_cell_ids_str[:3]}\")\n",
    "\n",
    "    # Filter centroids to match valid cell IDs\n",
    "    keep_mask = np.array([cid in valid_cell_ids_set for cid in all_cell_ids_str])\n",
    "    centroids_um = all_centroids_um[keep_mask]\n",
    "    cell_ids = np.array([all_cell_ids[i] for i in np.where(keep_mask)[0]])\n",
    "\n",
    "    # Debug: Print the number of matched cells\n",
    "    print(f\"→ Debug: Number of matched cells: {len(centroids_um)}\")\n",
    "\n",
    "    # Check if we have matching cells\n",
    "    if len(centroids_um) == 0:\n",
    "        # Debug: Print unmatched IDs for further investigation\n",
    "        unmatched_ids = set(all_cell_ids_str) - valid_cell_ids_set\n",
    "        print(f\"→ Debug: Unmatched IDs (sample): {list(unmatched_ids)[:10]}\")\n",
    "        raise ValueError(\"No cells matched between filtered AnnData and nucleus boundaries. Check cell ID formats.\")\n",
    "    \"\"\"\n",
    "    print(f\"✔ Extracted {len(centroids_um)} valid centroids after MAD filtering\\n\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Step 3: Align Xenium coordinates to H&E image\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Use the updated alignment function that checks for existing transformation\n",
    "    print(\"→ Starting alignment process…\")\n",
    "    coords_px, affine = align_xenium_to_he(\n",
    "        sdata=sdata,\n",
    "        he_image=he_image,\n",
    "        centroids_um=centroids_um,\n",
    "        psx=psx,\n",
    "        psy=psy,\n",
    "        xenium_dir=xenium_dir  # Make sure xenium_dir is defined or passed as a parameter\n",
    "    )\n",
    "    print(\"✅ Alignment completed successfully\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    # Convert physical coordinates (microns) to pixel coordinates\n",
    "    print(\"→ Converting centroids from µm → pixels …\")\n",
    "    coords_px = np.zeros_like(centroids_um, dtype=int)\n",
    "    coords_px[:, 0] = np.round(centroids_um[:, 0] / psx).astype(int)\n",
    "    coords_px[:, 1] = np.round(centroids_um[:, 1] / psy).astype(int)\n",
    "    print(\"✔ Sample pixel coords:\", coords_px[:5], \"\\n\")\n",
    "    \n",
    "    # Clip coordinates to image boundaries\n",
    "    # Why clip coordinates?\n",
    "    # Answer: Prevents out-of-bounds errors when extracting patches.\n",
    "    # Some cells might be detected slightly outside the imaged area.\n",
    "    h, w = he_image.shape[:2]\n",
    "    coords_px[:, 0] = np.clip(coords_px[:, 0], 0, w - 1)\n",
    "    coords_px[:, 1] = np.clip(coords_px[:, 1], 0, h - 1)\n",
    "    \n",
    "    # Sample cells using grid-based approach\n",
    "    # Why sample instead of using all cells?\n",
    "    # Answer: Processing all cells would be computationally expensive and unnecessary\n",
    "    # for quality assessment or method development. Grid sampling ensures spatial\n",
    "    # diversity while keeping the sample size manageable.\n",
    "    sample_idx = sample_grid_cells(coords_px, grid_size=8, cells_per_grid=5)\n",
    "    print(f\"→ Sampled {len(sample_idx)} cells\\n\")\n",
    "    \n",
    "    # Create visual preview of sampled cells\n",
    "    create_downsampled_preview(he_image, coords_px, sample_idx)\n",
    "    \n",
    "    # Extract patches only for sampled cells and assess quality\n",
    "    # Extract and enhance patches without quality assessment\n",
    "    print(\"→ Extracting patches for sampled cells …\")\n",
    "    test_idxs = sample_idx[:min(50, len(sample_idx))]  # Limit for testing/visualization\n",
    "    enhanced_patches, normalized_patches = extract_image_patches_enhanced(\n",
    "        he_image, coords_px[test_idxs]\n",
    "    )\n",
    "\n",
    "    # Create dummy quality results: all valid initially\n",
    "    quality_results = {}\n",
    "    kappas = [0.5, 0.75, 1.0, 1.5, 2.0]\n",
    "    for cid in enhanced_patches:\n",
    "        quality_results[cid] = [\n",
    "            {'kappa': k, 'score': 1.0, 'is_valid': True, 'reason': 'Valid'}\n",
    "            for k in kappas\n",
    "        ]\n",
    "\n",
    "    # Apply black-patch QC at kappa=2.0 only\n",
    "    final_patches, final_quality = paper_specific_qc(enhanced_patches, quality_results)\n",
    "\n",
    "    # Skip QC summary, distribution plots, etc.\n",
    "\n",
    "    # (Optional) Just show a few patches without filtering by score\n",
    "    print(f\"✔ Extracted patches for {len(final_patches)} cells\\n\")\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    for i, (cid, patches_list) in enumerate(list(final_patches.items())[:10]):\n",
    "        patch = patches_list[-1]  # Show κ=2.0 as an example\n",
    "        axes[i].imshow(patch)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Cell {cid}\\nκ=2.0\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Calculate and display total processing time\n",
    "    print(f\"\\nTotal processing time: {time.time() - t_start:.1f}s\")\n",
    "    \n",
    "    # Return all extracted data for potential downstream use\n",
    "    # Why return a comprehensive dictionary?\n",
    "    # Answer: Allows flexibility for different downstream analyses,\n",
    "    # debugging, or further processing without rerunning the entire pipeline.\n",
    "    return {\n",
    "        'centroids_px': coords_px,\n",
    "        'cell_ids': cell_ids,\n",
    "        'sample_indices': sample_idx,\n",
    "        'enhanced_patches': final_patches,\n",
    "        'normalized_patches': normalized_patches,\n",
    "        'quality_results': final_quality,\n",
    "    }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Optionally preload data once\n",
    "    sdata_cache, he_image_cache, psx_cache, psy_cache = load_data()\n",
    "    # Then reuse caches on subsequent calls\n",
    "    results = main_patch_extraction_pipeline(\n",
    "        sdata=sdata_cache,\n",
    "        he_image=he_image_cache,\n",
    "        psx=psx_cache,\n",
    "        psy=psy_cache\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
